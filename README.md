# Benchmarking Retrieval-Supported Large Language Models for Open-Domain Question Answering

This project implements and evaluates various Information Retrieval (IR) techniques for Retrieval-Augmented Generation (RAG) systems. It compares traditional lexical search against semantic, hypothetical (HyDE), and agentic retrieval strategies (PRF, RAG-Fusion) using MS MARCO Question Answering dataset.

## ðŸš€ Implemented Techniques
1. **Lexical Retrieval:** Sparse keywoard search using **BM25 Okapi**
2. **Semantic Retrieval:** Dense vector using **Sentence Transformers** (`all-MiniLM-L6-v2`) and Cosine Similarity
3. **HyDE (Hypothetical Document Embeddings):** Generates a hypothetical answer using an LLM to ground the vector search
4. **Pseudo-Relevance Feedback (PRF):** Uses an LLM to analyze initial results and rewrite the search query for better precision.
5. **RAG-Fusion (RRF):** Generates multiple query perspectives and re-ranks results using **Reciprocal Rank Fusion**

## ðŸ“‚ Project Structure
```text
.
â”œâ”€â”€ data/                       # Dataset storage
â”‚   â”œâ”€â”€ dev_v1.1.json           # Raw MS MARCO dataset
â”‚   â””â”€â”€ ms_marco_qna_dataset.csv # Parsed CSV used by the pipeline
â”œâ”€â”€ outputs/                    # Generated answers and evaluation metrics
â”œâ”€â”€ retrieval_techniques/       # Logic for specific retrieval strategies
â”‚   â”œâ”€â”€ lexical_retrieval_based_conf.py
â”‚   â”œâ”€â”€ semantic_retrieval_based_conf.py
â”‚   â”œâ”€â”€ hyde_retrieval_based_conf.py
â”‚   â”œâ”€â”€ pseudo_relevance_feedback_based_retrieval_conf.py
â”‚   â””â”€â”€ rag_fusion_based_retrieval_conf.py
â”œâ”€â”€ evaluation/                 # Metrics and Judging scripts
â”‚   â”œâ”€â”€ evaluate_confs.py       # BLEU, ROUGE, BERTScore
â”‚   â”œâ”€â”€ llm_as_judge.py         # LLM-as-a-Judge (GPT-4o)
â”‚   â””â”€â”€ calculate_avg_metrics.py # Final averaging script
â”œâ”€â”€ utils/                      # Helper utilities
â”‚   â”œâ”€â”€ model_factory.py        # LLM Factory
â”‚   â””â”€â”€ ms_marco_qna_dataset_parser.py
â”œâ”€â”€ main.py                     # Entry point for retrieval experiments
â””â”€â”€ requirements.txt            # Dependencies
```

## Setup & Installation

### Clone the Repository

```bash
git clone https://github.com/GokayGulsoy/CENG-543-Information-Retrieval-Project.git
cd CENG 543 Information Retrieval Course Project
```

### Install Dependencies

Ensure you have Python 3.10+ installed

```bash
pip install -r requirements.txt
```

### Set Environment Variables

Create a `.env` file or set the following variables in your terminal for the LLM provider you intend to use. If you want persistent approach, then set environment variables system wide.

```bash
# Windows (Poweshell)
$env:OPENAI_API_KEY="sk-..."

#Mac/Linux
export OPENAI_API_KEY="sk-..."
```

## Data Preparation 

1. Download the MS [MARCO Question Answering dataset](https://microsoft.github.io/msmarco/).
2. Place the `dev_v1.1.json` file inside the data directory
3. Run the parser from the **root directory** to generate the CSV file (subset of MSC MARCO QNA dataset)


```bash
python utils.ms_marco_qna_dataset_parser
```

## Usage & Execution Order

To perform a complete experiment, you must run the scripts in the following order.

### Step 1: Run Retrieval Experiment

Generates answers for the dataset using a specific technique

```bash
python main.py --technique <TECHNIQUE> --llm-model-id <MODEL>
```

Available Techniques: `lexical` | `semantic` | `hyde` | `prf` | `rrf` 

### Critical Dependency for PRF 

The **Pseudo-Relevance Feedback (PRF)** technique relies on the initial context provided by the Semantic Retrieval. You **must** run the semantic retrieval technique before running PRF.

```bash
# first, run the semantic retrieval to generate the base results 
python main.py --technique semantic --llm-model-id <MODEL>

# Then, run PRF (it reads outputs/ms_marco_qna_with_generated_answers_semantic.csv)
python main.py --technique prf --llm-model-id <MODEL>
```

### Example (RAG Fusion)

```bash
python main.py --technique rrf --llm-model-id gpt-3.5-turbo
```

## Step 2: Calculate Standard Metrics

Computes traditional NLP metrics (BLEU,ROUGE,BERTScore).

```bash
python evaluation.evaluate.py --retrieval-technique <TECHNIQUE>
```

Output: `outputs/ms_marco_qna_with_generated_answers_metrics_<TECHNIQUE>.csv`

## Step 3: Run LLM-as-a-Judge

Uses GPT-4o to grade answers on **Correctness**, **Faithfulness**, and **Context Quality** (Scale 1-5)

```bash
python evaluation.llm_as_judge --retrieval-technique <TECHNIQUE>
```

Output: `outputs/ms_marco_qna_with_generated_answers_metrics_judge_scores_<TECHNIQUE>.csv`

## Step 4: Calculate Final Averages

Aggregates all scores into final report.

```bash
python evaluation.calculate_avg_metrics --retrieval-technique <TECHNIQUE>
```

### Evaluation Metrics Explained

- BLEU / ROUGE: Measures lexical overlap with ground truth answers.
- BERTScore: Measures semantic similarity using contextual embeddings.
- LLM Judge Correctness: Does the AI answer convey the same meaning as the human answer?
- LLM Judge Faithfulness: Is the answer derived only from the retrieved context (hallucination check)?
- LLM Judge Context Quality: Did the retrieval step find the relevant information?


Research Paper Link: [Benchmarking Retrieval-Supported Large Language Models for Open-Domain Question Answering](CENG543_Research_Project_Paper.pdf)
